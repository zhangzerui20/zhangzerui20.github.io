<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[实现一个RPC框架（六）网络通信和通信协议设计]]></title>
      <url>%2F2017%2F03%2F10%2Frpc%2Frpc_4%2F</url>
      <content type="text"><![CDATA[实现一个RPC框架（六）网络通信和通信协议设计前言​ 前面主要讲解了通信层之上的逻辑设计。这节主要讲一下本RPC框架的网络通信层和协议的设计。其中网络通信使用了Netty库，Netty是一个NIO的网络库。 1. 通信协议设计​ 从前面也可以看出服务器和客户端通信主要使用的是RpcRequest和RpcResponse两个类。 1.1. RpcRequest和RpcResponse类​ RpcRequest类为rpc请求的封装类，首先我们站在RPC服务器的角度分析一下request都需要哪些信息，RPC服务器收到一个请求，肯定是根据这个请求，反射出Method对象，并传入已经装配好的接口实现类，并传入调用的参数，返回正确的结果。所以RpcRequest需要以下的信息： RPC服务接口的类型名，在服务端通过Class.forName()得到Class对象。 RPC调用的函数名，Class对象通过函数名得到对应的函数。 RPC调用的函数的参数类型“们”，得到函数类型需要。 RPC调用的函数的参数“们”，invoke时传入。 RPC的requestID ​ 前面几个参数上面已经讲解过了，注意最后一个参数。为什么需要一个RequestID呢，因为请求通过网络进行传输，很有可能第二个请求的回应先于第二个请求的回应到达，这个时候我们需要一个ID来表示到底是哪一个请求的回应。 123456789101112public class RpcRequest &#123; //为什么需要一个requestId //本框架有两种版本，同步版本和异步版本，在异步版本中，可能发出多条RPC请求，并且受到乱序的RPC回应，这样必须在request和response中都包含一个requestID private String requestID; //下面四个成员，标示了一个特定的方法，服务器端可以使用这些参数来调用特定的方法。 private String className; private String methodName; private Class&lt;?&gt;[] parameterTypes; private Object[] parameters; ... ... //get()和set()方法&#125; ​ RpcResponse类则，比较简单，先贴出代码实现： 123456789public class RpcResponse &#123; private String requestID; //RPC框架需要处理错误 private Throwable error; //处理的返回值 private Object result; ... ... //get() 和 set()方法 &#125; ​ 需要注意的是，除去需要id来标示一个请求，和返回RPC调用对应的结果外。还需要设置一个Throwable域。Throwable是Java异常体系的基类。当处理RPC请求的过程中发生异常的时候，需要设置这个域，返回异常。 1.2. 网络通信的实现​ 这里，我从服务器和客户端的角度，分别讲一下本RPC框架的网络通信过程。 1.2.1 服务器部分​ 服务器端为Netty添加了三个handler。分别是： RpcDecoder：继承了ByteToMessage抽象类，收到字节流之后，将字节流经过反序列化转为Request对象。 RpcHandler：Rpc的处理逻辑类，从RpcRequest中获取必要的信息，并反射调用对应的方法，得到结果，将结果封装成RpcResponse并写入到管道。 RpcEncoder：继承了MessageToByteEncoder，将写出的RpcResponse序列化成字节流并发出。 Netty的原理不再详细叙述了，这里主要说一下序列化的实现。 当需要通过socket传输数据的时候，比较常见的两种做法是： 把对象包装成JSON字符串传输。 采用java对象的序列化和反序列化。 ​ 序列化得方式有，java的原生序列化，谷歌的protoBuf，和protostuff等。由于java的原生序列化速度很慢，而protoBuf又需要编写.proto文件，这里我选用了protostuff。说protostuff比java原生的序列化速度快，有证据么？我就做了一个测试。 ​ 首先，创建一个10个属性的对象，使用java原生序列化，并反序列化1000次： 12345678910111213141516long startTime = System.currentTimeMillis();for(int i = 0;i &lt; 1000 ;i++)&#123; Test test = new Test(1, 2.0, "zcr", 3, 4.0, 5, "test", 6, 7.0, "dd"); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(new File("test_file"))); oos.writeObject(test); oos.flush(); oos.close(); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(new File("test_file"))); Test t = (Test) ois.readObject(); ois.close(); //t.fun();&#125;long endTime = System.currentTimeMillis();System.out.println("get time " + (endTime - startTime)); ​ 结果是 1get time 3202 ​ 而使用protostuff时 123456789101112131415161718192021222324252627long startTime = System.currentTimeMillis();for(int i = 0;i &lt; 1000; i++)&#123; Test test = new Test(1, 2.0, "zcr", 3, 4.0, 5, "test", 6, 7.0, "dd"); byte[] bytes = SerializationUtil.serialize(test); FileOutputStream out = null; try &#123; out = new FileOutputStream("test_file"); out.write(bytes); out.close(); &#125; catch (FileNotFoundException e) &#123; System.out.println("no get the filesdf"); e.printStackTrace(); &#125; Test t1; byte[] bb = new byte[bytes.length]; FileInputStream in = new FileInputStream(new File("test_file")); int len = in.read(bb); //System.out.println(len); in.close(); t1 = SerializationUtil.deserialize(bb, Test.class); //t1.fun();&#125;long endTime = System.currentTimeMillis();System.out.println("get time " + (endTime - startTime)); ​ 得到的结果是： 1get time 635 ​ 可以发现，速度确实快了很多，这里我为protostuff封装了类SerializationUtil。SerializationUtil里面封装了序列化和反序列化方法。直接贴出来使用的方法，使用比较简单。 1234567891011121314151617181920212223242526272829303132333435363738public class SerializationUtil &#123; //构建schema是一个非常耗时的行为，所以我们为他建立缓存 //为protostuff的schema(模式)建立缓存 private static Map&lt;Class&lt;?&gt;, Schema&lt;?&gt;&gt; cachedSchema = new ConcurrentHashMap&lt;Class&lt;?&gt;, Schema&lt;?&gt;&gt;(); private static Objenesis objenesis = new ObjenesisStd(true); //得到一个类型的schema模式的方法 private static &lt;T&gt; Schema&lt;T&gt; getSchema(Class&lt;T&gt; cls)&#123; Schema&lt;T&gt; schema = (Schema&lt;T&gt;) cachedSchema.get(cls); if (schema == null) &#123; //如果缓存中没有 schema = RuntimeSchema.createFrom(cls); if (schema != null) &#123; cachedSchema.put(cls, schema); &#125; &#125; return schema; &#125; //序列化的方法，将一个对象转为字节流 public static &lt;T&gt; byte[] serialize(T obj)&#123; Class&lt;T&gt; cls = (Class&lt;T&gt;) obj.getClass(); LinkedBuffer buffer = LinkedBuffer.allocate(LinkedBuffer.DEFAULT_BUFFER_SIZE); try&#123; Schema&lt;T&gt; schema = getSchema(cls); return ProtostuffIOUtil.toByteArray(obj, schema, buffer); &#125;finally &#123; buffer.clear(); &#125; &#125; //反序列化方法，将一个字节序列转化为对象 public static &lt;T&gt; T deserialize(byte[] data, Class&lt;T&gt; cls)&#123; T message = (T)objenesis.newInstance(cls); Schema&lt;T&gt; schema = getSchema(cls); ProtostuffIOUtil.mergeFrom(data, message, schema); return message; &#125; &#125; ​ 其中，在反序列化时候，需要将从字节得到的信息反射出一个对象。这里使用了objenesis库进行反射。当然也可以使用java原生的反射机制。 1.2.2. 客户端部分​ 客户端实现部分与服务器端差不多，就不再赘述了，感兴趣的可以从我的github中clone代码来看。 总结​ 至此，整个RPC框架就讲解完毕了，关于一个RPC的重点问题，1.服务发现，2.透明实现，3.序列化等我都做了详细的描述。做完之后，我也算是对RPC流程有了一个大概的了解，接下来准备看看dubbo，看看真正的RPC框架是怎么实现的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[实现一个RPC框架（无）客户端的实现 2]]></title>
      <url>%2F2017%2F03%2F10%2Frpc%2Frpc_3%2F</url>
      <content type="text"><![CDATA[实现一个RPC框架（五）客户端的实现 2前言​ 继续上篇，这节主要讲述RPC的调用过程。调用过程大致流程为： 从调用中获取到接口信息，函数信息，并封装成RpcRequest对象。 将RPC对象通过网络发出。 等待服务器处理并返回RpcResponse，从RpcResponse中获取到result。 这小节一共分为两部分： RPC调用的异步实现。 RPC调用的同步实现。 此刻，小伙伴们一定会好奇地说，嗨呀，张帅啊，为什么要先讲异步的呢？同步的不是更好实现么？ 其实本框架是在异步的基础上实现同步的。所以，接着看咯。 1. RPC调用的异步实现​ 异步调用的实现思路是这样的： 构造了一个RpcFuture对象，这个对象有一个RpcResponse类的域。 当发起一次RPC调用的时候，会产生一个RequestID(整个程序中独有)，并初始化一个RpcFuture，以ID为键，以future为值，将它们存入到一个Map中，并且将这个future返回给用户。 服务器端接收到request并处理完成后，将request的ID保存到response中并通过网络发回到客户端。 客户端收到response后，从中获取到id，通过id又获取到对应的future，给future的response域设置成当前这个返回值。 客户调用future.get()获取到这个response中的结果。 1.1. RpcFuture类​ 所以，首先我们先来讲讲这个RpcFuture类。先给大家看看这个类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class RpcFuture implements Future&#123; private RpcResponse response; private Sync sync; public RpcFuture()&#123; sync = new Sync(); &#125; public boolean isDone() &#123; return sync.isDone(); &#125; public Object get() throws InterruptedException, ExecutionException &#123; sync.acquire(-1); //这里acquire就相当于已经获得了锁了，所以下面这个直接使用 if(this.response != null)&#123; return this.response.getResult(); &#125;else&#123; return null; &#125; &#125; public Object get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; boolean success = sync.tryAcquireNanos(-1, unit.toNanos(timeout)); if(success)&#123; if(response != null)&#123; return response; &#125;else&#123; return null; &#125; &#125; return null; &#125; //读取到rpc服务器的回应信息后，需要调用这个函数设置response到对应的request中 public void done(RpcResponse response)&#123; this.response = response; sync.release(1); &#125; static class Sync extends AbstractQueuedSynchronizer&#123; //定义，RpcFuture的整形的状态 private final int done = 1; private final int pending = 0; @Override protected boolean tryAcquire(int arg) &#123; if (getState() == 1) &#123; return true; &#125; else return false; &#125; @Override protected boolean tryRelease(int arg) &#123; if (getState() == pending) &#123; if(compareAndSetState(pending, done)) return true; &#125; return false; &#125; public boolean isDone()&#123; return getState() == done; &#125; &#125;&#125; ​ 首先RpcFuture类实现了Future接口。熟悉Java线程池的小姐姐们肯定知道调用线程池的submit方法，会返回一个future对象，用来得到、控制线程的状态。我们这里的RpcFuture的作用也跟Future很想象，我们通过RpcFuture来了解一次RPC调用是否完成，并得到结果。 ​ 通过异步调用的过程，我们可以知道RpcFuture需要被多个线程访问，首先客户端的网络读取线程，会去设置RpcResponse域，还可能会有多个线程在阻塞调用future.get()，得到获取结果。可以发现，这有点类似于Java中的闭锁机制，当一个线程读取线程设置完response(countDown())之后，多个调用get()函数(await)的线程便可以解除阻塞。为了实现RpcFuture的阑珊效果，我使用了AQS机制。 ​ AQS机制，是java提供的制作同步器的框架。使用AQS的步骤如下： 创建一个继承自AbstractQueuedSynchronizer抽象类的内部类。 在这个内部类中实现AbstractQueuedSynchronizer的tryAcquire和tryRelease方法。 实例化这个内部类syn，在需要”获取“的地方调用syn.acquire()，需要”释放“的地方调用syn.release()。 ​ 后面说的future，都指的是本框架中的RpcFuture。 ​ 主要分析一下，future的get()方法和done()方法。AQS管理了一个整数状态。在future中状态表示请求的处理状态，分为pending = 0，表示未完成。done = 1，表示已经完成。对future来说get()方法，不会影响其他的线程的get()，所以，在get()方法中不会改变状态，而只是判断状态是否为done。而在done()方法中，需要更改将原来的pending状态更改为done。 1.2 异步调用的工作流程​ 异步调用余下的技术比较简单，我就描述一下多个类之间的协作流程： 使用RpcProxy类的createAsyn方法(需传入接口的类型)创建一个RpcCaller对象，调用caller对象的call()方法，传入方法名和参数。 得到参数的类型“们”，使用接口的类型进行反射，得到特定方法的Method对象。 通过Method对象构造RpcRequest对象。 从ConnectManage中获取一个到服务器的连接，并使用这个handler发送Rpc请求。把这个请求对应的id和future存入Map中，返回一个future给用户。 用户调用future.get()，阻塞获取结果。其中get提供了超时版，超时就返回一个null。 2. 同步RPC调用的实现​ 同步RPC调用，即用户阻塞在这个调用上，等待框架返回一个结果。因为这样调用直接返回我们需要的结果，而不是异步调用的future，我们可以实现更好的透明化。这里使用了动态代理，实现透明的RPC调用。 ​ 动态代理是jdk提供的用来对委托类进行访问控制的机制。当用动态代理的产生的对象去调用一个方法的时候，这个方法调用，就会被转发给实现了InvocationHandler接口类的invoke方法中。 ​ 代理类的实现很简单，总共需要两步： 调用Proxy的静态方法newProxyInstance()方法。 传入类加载器、接口接口列表、一个实现了InvocationHandler接口的对象。 ​在本框架中，我们为RPC调用的接口创建动态代理对象。当调用接口的方法的时候，这个调用会被转发到代理中。在代理中进行发送RpcRequest并返回结果。 ​这里贴出主要的代码： 12345678910111213141516171819T t = (T)Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class&lt;?&gt;[]&#123;interfaceClass&#125;, new InvocationHandler() &#123; //这个方法是反射出来的，当被代理的接口的方法被调用的时候调用的方法 public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //在这个方法中，connect RPC服务器，并且发送请求，期待回应 RpcRequest req = new RpcRequest(); long id = reqID.getAndIncrement(); req.setRequestID(Long.toString(id)); req.setClassName(method.getDeclaringClass().getName()); req.setMethodName(method.getName()); req.setParameterTypes(method.getParameterTypes()); req.setParameters(args); RpcClientHandler handler = ConnectManage.getInstance().chooseHandler(); RpcFuture future = handler.sendRequest(req); return future.get(); &#125;&#125;); ​ 至于同步的实现，这里就是不给用户返回future了，而是直接调用future.get()，阻塞等待获取服务器发来的结果。 ​ 关于动态代理还有一点我想说的，动态代理只能代理接口，如果想对一个特定的类进行代理，则应该这样： 这个委托类必须实现一个接口。 对这个接口进行动态代理。 向实现InvocationHandler接口的类的构造函数传入委托类，并保存起来。 ​这样就可以在invoke方法中使用委托类了。 总结​ 本小节描述了，RPC同步调用和异步调用的逻辑实现，下一小节讲解网络通信和通信协议的设计。 参考https://www.ibm.com/developerworks/cn/java/j-lo-proxy1/index.html 《java并发编程实践》 p254]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[实现一个RPC框架（四）客户端的实现 1]]></title>
      <url>%2F2017%2F03%2F10%2Frpc%2Frpc_2%2F</url>
      <content type="text"><![CDATA[实现一个RPC框架（四）客户端的实现 1前言​ 在客户端，用户可以使用几句简单的代码，得到远程调用的结果。在客户端，有两种方式来进行远程调用，同步和异步。首先先展示一下调用的用户： 12345678910111213//框架的配置ServiceDiscovery discovery = new ServiceDiscovery("127.0.0.1:2181");RpcProxy proxy = new RpcProxy(discovery);//同步调用TestService hello = proxy.create(TestService.class);String res = hello.helloService("trecool");System.out.println(res);//异步调用RpcCaller caller = proxy.createAsyn(TestService2.class);RpcFuture future = caller.call("add", 5, 7);int resa = (Integer) future.get(); ​ RPC调用的粗略工作过程如下： 获取RPC服务的地址。 将用户的调用封装成请求，以特定的协议发送给RPC服务器。 从RPC服务器返回的回应中得到函数运行的结果，并返回给用户。 在客户端，可能会有多个线程在使用RPC框架，所以，所有的公用的数据对象都需要进行保护。 如往常一样，我还是通过模块划分来讲解客户端的现实，客户端具体实现了以下几个模块： RPC服务的发现和管理。 与RPC服务器的连接的管理。 动态代理、同步调用的实现。 自制异步框架的实现。 ​ 1. RPC服务的发现和管理​ 这个模块的实现类是ServiceDiscovery，主要的工作一共包括两点： 连接zk，得到RPC服务的地址“们”，将其存入list。 当zk节点变化时，主动通知客户端进行更新。 1.1. 连接、获取RPC服务器的地址“们”​ 所谓的RPC服务发现，就是找到服务器的地址。从前一章，我们知道RPC服务的地址都是被注册在zookeeper服务器上的。所以这里的发现，就是从zookeeper的节点中取出所有的RPC的服务的地址，并缓存在list中。这里调用zk.getChildren()，得到zk的节点。 1.2. zk节点变化时的更新​ 其次，RPC服务的地址发生变化时，我们需要得到zk服务器发来的信息，这样我们可以及时更改我们的缓存的list，使得用户可以访问到正确的RPC服务器。为了实现这个功能，我们在获得zk节点的时候，向其注册一个watcher，并且监视zk的节点变更的事件，当zk节点变更时，会调用我们在watcher中设置的回调，在回调中我们又调用更新节点的函数，这样就可以动态更新客户端的关于RPC服务的缓存。关键代码如下： 123456789101112131415161718private void watchNode(final ZooKeeper zk)&#123; //这里得到所有node的全路径，并注册watcher，监听node change事件,用来动态更新 List&lt;String&gt; nodeList = zk.getChildren(Constant.ZK_REGISTRY_PATH, new Watcher()&#123; public void process(WatchedEvent event) &#123; if(event.getType() == Event.EventType.NodeChildrenChanged)&#123; watchNode(zk); &#125; &#125; &#125;); //调用api从路径名得到node的内容 List&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); for(String node : nodeList)&#123; byte[] bytes = zk.getData(Constant.ZK_REGISTRY_PATH + "/" + node, false, null); dataList.add(new String(bytes)); &#125; ConnectManage.getInstance().updateConnectedServer(dataList); this.dataList = dataList;&#125; 2. 与RPC服务器的连接的管理​ 这个模块主要的实现类是ConnectManager，主要的工作是： 抽象出RPC连接。 缓存连接。 当用户请求时，负载均衡的返回一个连接。 管理连接变化。 2.1. 抽象出连接​ 若每次RPC调用都创建一个连接，会减慢调用的速度，本框架中，我将RPC连接抽象出来，缓存在全局的list中。至于如何抽象这个连接，我是这样考虑的，框架需要对这个连接进行读写，因为使用了Netty，读函数肯定在handler中，而想Netty一般是向channel中写。所以，我们在Netty的客户端的逻辑处理handler中，注册一个channelRegistered方法，在这个方法中将代表与RPC服务的channel存下来，并导出一个public的sendRequest方法，在这个方法中向channel中写数据。所以这里的RpcClientHandler就代表了一个与RPC服务的连接，RpcClient的概要定义如下： 12345678910111213141516public class RpcClientHandler extends SimpleChannelInboundHandler&lt;RpcResponse&gt; &#123; private volatile Channel channel; ... ... //其他域 protected void channelRead0(ChannelHandlerContext ctx, RpcResponse msg)&#123; ... ... //读取方法 &#125; public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; super.channelRegistered(ctx); channel = ctx.channel(); &#125; public RpcFuture sendRequest(RpcRequest req)&#123; ... ... //发送函数 &#125; // 其他方法&#125; 2.2 缓存连接​ 首先需要明确的是，这个缓存是一个全局的对象，而且这个缓存会被多个线程访问，这里的多个线程包括，RPC服务器地址变更时的需要更改缓存的线程(写操作)，和用户请求一个RPC连接的线程(读)。这里很明显是一个，读发生的次数大于写的情况，所以这里我采用了CopyOnWriteArrayList，用作缓存的数据结构。 ​ CopyOnWrite叫做写时复制容器，是一种并发容器。其原理大概是，多个线程可以同时读这个结构，而当其中一个线程想要写这个结构的时候，它会将容器复制一份拷贝，再对这个拷贝进行写，随后再将原来结构的引用指向这个拷贝。是一种延时懒惰策略。 ​ 当ServiceDiscovery中的watchNode在回调中被触发的时候，会调用updateConnectedServer()函数通过更新的zk节点的list来更新我们的连接缓存。根据传入的datalist，更新操作包括两步，1.增加缓存list中没有的连接，这里需要为新的RPC服务地址建立Netty连接，并且为connect函数注册listener，在operationComplete方法中，将逻辑处理handler(代表一个连接)存入到缓存中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public void updateConnectedServer(List&lt;String&gt; allServerAddress) &#123; if (allServerAddress != null) &#123; if (allServerAddress.size() &gt; 0) &#123; //先将String的address转为InetSocketAddress HashSet&lt;InetSocketAddress&gt; newAllServerNodeSet = new HashSet&lt;InetSocketAddress&gt;(); for (int i = 0; i &lt; allServerAddress.size(); ++i) &#123; String[] array = allServerAddress.get(i).split(":"); if (array.length == 2) &#123; //检查IP合法性 String host = array[0]; int port = Integer.parseInt(array[1]); final InetSocketAddress remotePeer = new InetSocketAddress(host, port); newAllServerNodeSet.add(remotePeer); &#125; &#125; //添加新的连接 for (final InetSocketAddress serverNodeAddress : newAllServerNodeSet) &#123; if (!connectedServerNodes.keySet().contains(serverNodeAddress)) &#123; connectServerNode(serverNodeAddress); &#125; &#125; //关闭并remove掉多余连接 for (int i = 0; i &lt; connectedHandlers.size(); ++i) &#123; RpcClientHandler connectedServerHandler = connectedHandlers.get(i); SocketAddress remotePeer = connectedServerHandler.getRemoteAddr(); if (!newAllServerNodeSet.contains(remotePeer)) &#123; LOGGER.info("Remove invalid server node " + remotePeer); RpcClientHandler handler = connectedServerNodes.get(remotePeer); handler.close(); connectedServerNodes.remove(remotePeer); connectedHandlers.remove(connectedServerHandler); &#125; &#125; &#125; else &#123; //传入的是空的list，即没有可用的RPC服务 LOGGER.error("No available server node. All server nodes are down !!!"); for (final RpcClientHandler connectedServerHandler : connectedHandlers) &#123; SocketAddress remotePeer = connectedServerHandler.getRemoteAddr(); RpcClientHandler handler = connectedServerNodes.get(remotePeer); handler.close(); connectedServerNodes.remove(connectedServerHandler); &#125; connectedHandlers.clear(); &#125; &#125; &#125; ​ 2.3. 均衡负载地返回一个连接​ 当用户请求时，从连接缓存中返回一个连接。这里采用的均衡负载的方法是轮询，按缓存中的顺序依次返回。这里需要注意的是，当缓存中没有连接的时候，需要等待条件变量，等待有新的RPC服务注册。这里使用可重入锁和条件变量，并在有新的连接增加时，signalAll()，来唤醒所有的等待线程。 1234567891011private ReentrantLock lock = new ReentrantLock();private Condition connected = lock.newCondition();private boolean waitingForHandler() throws InterruptedException &#123; lock.lock(); try &#123; return connected.await(this.connectTimeoutMillis, TimeUnit.MILLISECONDS); &#125; finally &#123; lock.unlock(); &#125;&#125; 2.4. 管理连接变化​ 当ServiceDiscovery中watchNode()函数在zk的watcher中被回调调用时，调用上面提到过的updateConnectedServer()函数更新节点即可。 总结​ 本小结描述了前两个模块的实现。包括服务发现，连接管理等。后面将讲解重点的RPC调用的实现。 参考：http://ifeve.com/java-copy-on-write/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[实现一个RPC框架 （三）从服务器写起]]></title>
      <url>%2F2017%2F03%2F10%2Frpc%2Frpc_1%2F</url>
      <content type="text"><![CDATA[实现一个RPC框架 （三）从服务器写起我们从服务器端开始写起。服务器端需要实现的功能有以下几点： 服务器端配置 注册RPC服务 rpc请求的处理 ​ 1. 服务器端配置服务器端配置包括两项： 1.1. RPC服务实现类的发现​ RPC服务以接口的方式提供给用户使用。在服务器这端需要将实现了RPC服务的类的对象缓存起来，这样当请求到来的时候才知道去调用哪个对象的对应方法。那么如何标注出RPC的实现类，并进行自动装载呢。这里使用注解加spring自动装载。 ​ 注解的声明如下： 123456@Target(&#123;ElementType.TYPE&#125;) //表示修饰此注解修饰类@Retention(RetentionPolicy.RUNTIME) //表示这个注解在运行时有效@Component //用于Spring的自动装配public @interface RpcService &#123; Class&lt;?&gt; value(); //注解参数为Class类型&#125; ​ 为什么使用一个Class的注解参数。因为收到的RPC请求中，肯定是发来一个RPC服务的接口名，我们需要用一个Map保存住接口名和实现类。所以用了Class参数来存接口的类型。这样，当Spring获取这个实现类的时候，也可以顺手获取到其实现的接口。 1.1. 装配Beans​ Server端共包含两个类： serviceRegistry类：用来将RPC服务器注册到Zookeeper节点上，这里需要装配一个zk服务器的地址。 rpcServer类：包含启动RPC服务，处理RPC主逻辑的类，需要装配serviceRegistry和服务器的地址。 2. 注册RPC服务​ 将RPC服务器的地址注册在中心服务器(ZK服务器)上，这样，客户端只用访问中心服务器。用Zookeeper来进行分布式管理和节点的负载均衡。 ​ 我将这里的功能分为了两点： 2.1. 向zk注册服务器地址​ 想zk注册节点的流程为： 创建zookeeper实例。 调用zk的create方法，创建一个临时的节点。 因为zk中的API基本都是异步的，所以不能当即返回结果，一般需要配合zk的watcher(zk中的回调机制)。举个例子，在连接创建zk对象(既连接zk服务器)的时候： 12345678910111213ZooKeeper zk = null;try &#123; //这里是书中的写法 //watcher是zookeeper中的回调机制 zk = new ZooKeeper(registryAddress, Constant.ZK_SESSION_TIMEOUT, new Watcher()&#123; //下面这个函数是，有事件发生时，执行的函数。 public void process(WatchedEvent event) &#123; if(event.getState() == Event.KeeperState.SyncConnected)&#123; latch.countDown(); &#125; &#125; &#125;); latch.await(); //latch初始化为1，这里阻塞等待连接成功 ​ 这里使用了watcher配合阑珊，保证了创建完再返回。具体zk的学习分析，后面会再出博客。这里就不赘述了。 2.2. 管理zk节点​ 当有RPC服务器更换地址，或者RPC服务器宕机，需要zk作出及时的响应。通知客户端访问新的地址。所以这里需要在客户端做处理，我先提出来，在客户端部分再详述。 3. RPC请求处理​ 在启动RPC服务器之前，需要进行如下的步骤。 3.1 将RPC服务接口和RPC服务器实现类作为键值对存入Map中​ 如上所述，为了通过用户发来的RPC服务器接口名来找到实现类，我们必须为RPC接口名和服务实现类做一个缓存。缓存的定义如下： 12//键为接口名，值为对应的实现类private Map&lt;String, Object&gt; handlerMap = new HashMap&lt;String, Object&gt;(); ​ 当Spring装配完所有的Bean之后，通过我们的RpcService，可以找到所有的RPC服务实现类和其对应的接口名。那么，我们现在需要在所有的bean装配完的时候，得到ApplicationContext，并从中获取到已经被实例化并初始化的RpcService Bean。 ​ Spring装配Bean的过程如下： 实例化; 设置属性值; 如果实现了BeanNameAware接口,调用setBeanName设置Bean的ID或者Name; 如果实现BeanFactoryAware接口,调用setBeanFactory 设置BeanFactory; 如果实现ApplicationContextAware,调用setApplicationContext设置ApplicationContext 调用BeanPostProcessor的预先初始化方法; 调用InitializingBean的afterPropertiesSet()方法; 调用定制init-method方法； 调用BeanPostProcessor的后初始化方法; ​ 因为我们需要获得applicationContext，所以在这里我们实现ApplicationContextAware接口，实现它的setApplicationContext方法。 12345678910public void setApplicationContext(ApplicationContext ctx) throws BeansException &#123; //这里得到所有带注解的bean Map&lt;String, Object&gt; serviceBeanMap = ctx.getBeansWithAnnotation(RpcService.class); if(!serviceBeanMap.isEmpty())&#123; for(Object bean : serviceBeanMap.values())&#123; String interfaceName = bean.getClass().getAnnotation(RpcService.class).value().getName(); handlerMap.put(interfaceName, bean); &#125; &#125;&#125; ​ 在这里，通过函数给我们提供的Application参数，可以获得所有的带有RpcService注解的类，并得到其注解参数中的接口类型。将接口名的字符串和对应的实现类存入到Map中。 3.2 启动RPC服务​ 当所有的bean装配完成的时候，我们需要启动一个TCP服务器，用来监听RPC请求。我们为服务器类实现了InitializingBean接口，并实现了其中的afterPropertiesSet()，在这里方法中，启动服务。 ​ 启动Netty服务器，设置Encoder和Decoder，设置处理RPC请求的Handler，然后将本Server地址注册到zk服务器上。启动Server的代码如下： 12345678910111213141516171819202122EventLoopGroup bossGroup = new NioEventLoopGroup(1); //负责监听的线程组EventLoopGroup workerGroup = new NioEventLoopGroup(); //负责io的线程组try&#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class).childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //设置channel和初始化的Handler @Override protected void initChannel(SocketChannel channel) throws Exception &#123; channel.pipeline() .addLast(new RpcEncoder(RpcResponse.class)) .addLast(new RpcDecoder(RpcRequest.class)) .addLast(new RpcHandler(handlerMap)); &#125;//添加了 解码、编码、处理逻辑的handler &#125;) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); String[] address = serverAddress.split(":"); int inetPort = Integer.parseInt(address[1]); ChannelFuture future = serverBootstrap.bind(inetPort).sync(); //ChannelFuture类似于JDK的并发包中的future，可以从ChannelFuture中获取本次执行的结果。 ​ 描述一下Netty启动服务器的原理。 ​ 首先创建的两个EventLoopGroup，是两个线程组。其中bossGroup是Accepter线程组，负责处理客户端的TCP连接，workerGroup则负责对每一个客户连接的IO操作。因为只服务只监听了一个端口，在这里将bossGroup的线程数设置为1。 ​ 接着为服务器设置了解码和编码、处理逻辑的handler。 ​ 服务器端的讲解到此为止。下一章会为大家讲述 客户端的基本配置和实现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[写一个RPC框架-总体架构]]></title>
      <url>%2F2017%2F02%2F19%2Frpc%2Frpc_fram%2F</url>
      <content type="text"><![CDATA[写一个RPC框架-总体架构​ 从今天起，本帅准备一步一步写一个面向java平台的分布式RPC框架。首先给出总体的流程图： ​ 其中，使用netty来进行NIO网络通信。使用Spring进行应用程序的配置。 ​ 使用Zookeeper，实现服务器端的服务注册，客户端的服务发现功能，这里便可实现将服务部署在多个节点。 ​ 序列化我选用Protostuff。接下来在每一章中再对每一块进行详细的描述。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[实现一个RPC框架]]></title>
      <url>%2F2017%2F02%2F18%2Frpc%2Frpc_intro%2F</url>
      <content type="text"><![CDATA[1.什么是RPC​ 一般我们写的程序，都是调用本地计算机的方法，这也称为本地过程调用。而RPC，全称是 Remote Procedure Call，叫做远程过程调用，是一种进程间通讯方式，允许程序员调用另一台主机上的函数。典型的实现，Java的RMI。 ​ RPC屏蔽了网络层，用户只关心函数的调用，网络传输对用户来说是透明的。这样RPC可以使用不同的协议，例如TCP、HTTP。 ​ RPC的工作流程如下图： ​ 一般来说，RPC由以下5个部分组成： client，客户端：调用远程调用方法。 Client stub，客户存根：负责将用户的调用封装成请求；把收到的服务器发来的运行结果接封装程序要的格式 RPCRuntime：负责网络传输。 Server stub，服务器存根：负责将用户的请求接封装，并调用server上特定的函数；把函数的返回值封装成结果。 Server，服务器端：实现RPC函数的接口，并将RPC服务暴露。 ​ ​ 当用户调用一个远程调用时，其实调用了User-stub，User-stub将用户的调用封装成请求，并发送给Server-stub，Server-stub将请求解封装并调用服务器上对应的函数。接着Server-stub将结果封装成Result结构并返回给用用户。 2.RPC的重点1.服务器端如何发布服务2.如何做到透明化远程调用​ 这里包括，消息的编码和解码、序列化、网络通信。 3.总结​ 简要的介绍了RPC是什么，并介绍了RPC的工作原理。接下来的博客里，将实现一个Java平台的RPC框架。]]></content>
    </entry>

    
  
  
</search>
